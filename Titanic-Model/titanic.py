# -*- coding: utf-8 -*-
"""titanic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aoOB-VtfyJKQt_r_swFVdU_dn9eBmX0k
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('titanic.csv')
df.head()

df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)

df.head()

numerical_features = df.select_dtypes(include = ['int', 'float']).columns.to_list()
categorical_features = df.select_dtypes(exclude = ['int', 'float']).columns.to_list()

print('Numerical Features:', numerical_features)
print('Categorical Features:', categorical_features)

continuous_features = [feature for feature in numerical_features if df[feature].nunique()>25]
discrete_features = [feature for feature in numerical_features if df[feature].nunique()<25]

print('Continuous Features:', continuous_features)
print('Discrete Features:', discrete_features)

df.isnull().sum()

plt.figure(figsize=(10,7))
sns.heatmap(df.isnull())
plt.show()

print("Percentage Missing: ",df['Age'].isnull().mean()*100)

print("Percentage Missing: ",df['Cabin'].isnull().mean()*100)

plt.figure(figsize=(20,7))
plt.subplot(1,2,1)
sns.histplot(df['Age'])
plt.subplot(1,2,2)
sns.distplot(df['Age'])
plt.show()

plt.figure(figsize=(20,7))
plt.subplot(1,2,1)
sns.histplot(df['Fare'])
plt.subplot(1,2,2)
sns.distplot(df['Fare'])
plt.show()

plt.figure(figsize=(30,7))
plt.subplot(1,4,1)
sns.countplot(df['Survived'])
plt.subplot(1,4,2)
sns.countplot(df['Pclass'])
plt.subplot(1,4,3)
sns.countplot(df['SibSp'])
plt.subplot(1,4,4)
sns.countplot(df['Parch'])
plt.show()

plt.figure(figsize=(30,7))
plt.subplot(1,3,1)
sns.countplot(df['Pclass'], hue=df['Survived'])
plt.subplot(1,3,2)
sns.countplot(df['SibSp'], hue=df['Survived'])
plt.subplot(1,3,3)
sns.countplot(df['Parch'], hue=df['Survived'])
plt.show()

plt.figure(figsize=(30,7))
plt.subplot(1,4,1)
sns.countplot(df['Sex'])
plt.subplot(1,4,2)
sns.countplot(df['Sex'], hue=df['Survived'])
plt.subplot(1,4,3)
sns.countplot(df['Embarked'])
plt.subplot(1,4,4)
sns.countplot(df['Embarked'], hue=df['Survived'])
plt.show()

plt.figure(figsize=(10,7))
sns.scatterplot(df['Age'], df['Fare'], hue=df['Survived'])
plt.show()

plt.figure(figsize=(30,7))
sns.pairplot(df, hue='Survived')
plt.show()

plt.figure(figsize=(30,7))
plt.subplot(1,2,1)
sns.violinplot(df['Age'])
plt.subplot(1,2,2)
sns.violinplot(df['Fare'])
plt.show()

plt.figure(figsize=(30,7))
plt.subplot(1,2,1)
sns.boxplot(df['Age'])
plt.subplot(1,2,2)
sns.boxplot(df['Fare'])
plt.show()

def outlier(feature):
  IQR = df[feature].quantile(0.75) - df[feature].quantile(0.25)
  LB = df[feature].quantile(0.25) - 1.5*IQR
  UB = df[feature].quantile(0.75) + 1.5*IQR
  print("LOWER BOUNDARY: ", LB)
  print("UPPER BOUNDARY:", UB)

print(df['Age'].describe())
outlier('Age')

len(df[df['Age']>64.8125])

print(df['Fare'].describe())
outlier('Fare')

len(df[df['Fare']>65.6344])

df['Age_NaN'] = np.where(df['Age'].isnull(), 1, 0)

df.head()

df[df['Age'].isnull()]

mean = df['Age'].mean()
df['Age'].fillna(mean, axis=0, inplace=True)

df.Age.isnull().sum()

df.Cabin.unique()

df.Cabin.nunique()

df.Cabin.isnull().sum()

df.Cabin.isnull().describe()

df['Cabin_NaN'] = np.where(df.Cabin.isnull(), 1, 0)

df.head()

df.Cabin.fillna('Missing', axis=0, inplace=True)

df.head()

df['Cabin'] = df.Cabin.str[0]

df.head()

df.Cabin.unique()

plt.figure(figsize=(25,7))
plt.subplot(1,2,1)
sns.countplot(df.Cabin)
plt.subplot(1,2,2)
sns.countplot(df.Cabin, hue=df['Survived'])
plt.show()

df_sex = pd.get_dummies(df['Sex'], drop_first=True)
df_sex.head()

df_cabin = pd.get_dummies(df['Cabin'])
df_cabin.head()

df.Embarked.unique()

df.Embarked.isnull().sum()

sns.countplot(df.Embarked)

df.Embarked.mode()[0]

df.Embarked.fillna(df.Embarked.mode()[0], axis=0, inplace=True)

df.Embarked.isnull().sum()

df_embarked = pd.get_dummies(df.Embarked)

df_embarked.head()

df_new = pd.concat([df, df_sex, df_cabin, df_embarked], axis=1)

df_new.head()

df_new.columns

df_new.drop(['Sex', 'Cabin', 'Embarked'], axis=1, inplace=True)
df_new.head()

df_new.corr()

plt.figure(figsize=(30,10))
sns.heatmap(df_new.corr(), annot=True)

df_new.head()

df_new.info()

plt.figure(figsize = (25,7))
plt.subplot(1,2,1)
sns.histplot(df_new['Age'])
plt.subplot(1,2,2)
sns.distplot(df_new['Age'])
plt.show()

X = df_new.drop('Survived', axis=1)
y = df_new['Survived']

X

y

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

order_rank = SelectKBest(score_func=chi2, k=12)
order_feature = order_rank.fit(X,y)

dfscores = pd.DataFrame(order_feature.scores_, columns = ["Score"])
dfcolumns = pd.DataFrame(X.columns)

features_rank = pd.concat([dfcolumns,dfscores],axis=1)

features_rank.columns = ['Features','Score']

features_rank

features_rank.nlargest(10, 'Score')

from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt

model=ExtraTreesClassifier()
model.fit(X,y)

print(model.feature_importances_)

ranked_features = pd.Series(model.feature_importances_, index = X.columns)

ranked_features

plt.figure(figsize = (20,8))
ranked_features.nlargest(19).plot(kind='barh', color='orange')
plt.show()

X = df_new[['male', 'Fare', 'Age', 'Pclass', 'SibSp', 'Parch']]
y = df_new['Survived']

X

y

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)

X_train.shape, y_train.shape, X_test.shape, y_test.shape

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train_scaled = sc.fit_transform(X_train)
X_test_scaled = sc.transform(X_test)

X_train_scaled

X_test_scaled

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Let's implement simple classifiers
classifiers = {
    "LogisiticRegression": LogisticRegression(),
    "KNearest": KNeighborsClassifier(),
    "Support Vector Classifier": SVC(),
    "DecisionTreeClassifier": DecisionTreeClassifier(),
    "RandomForestClassifier": RandomForestClassifier()
}

# Wow our scores are getting even high scores even when applying cross validation.
from sklearn.model_selection import cross_val_score


for key, classifier in classifiers.items():
    classifier.fit(X_train_scaled, y_train)
    training_score = cross_val_score(classifier, X_train_scaled, y_train, cv=5)
    print("Classifiers: ", classifier.__class__.__name__, "Has a training score of", round(training_score.mean(), 2) * 100, "% accuracy score")

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

log_reg_params = {"penalty": ['l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}
grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params, refit=True, verbose=4)
grid_log_reg.fit(X_train_scaled, y_train)
print(grid_log_reg.best_params_)
print(grid_log_reg.best_estimator_)

grid_log_reg_predictions = grid_log_reg.predict(X_test_scaled)
cm = confusion_matrix(y_test, grid_log_reg_predictions)
accuracy = accuracy_score(y_test, grid_log_reg_predictions)
class_report = classification_report(y_test, grid_log_reg_predictions)
sns.heatmap(cm, annot=True)
print('accuracy:', accuracy)
print('classification report:\n', class_report)

# svc_param = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']} 
svc_param = {'C': [0.1, 1, 10], 'gamma': [1, 0.1, 0.01], 'kernel': ['rbf']} 
grid_svc = GridSearchCV(SVC(), svc_param, refit=True, verbose=4)
grid_svc.fit(X_train_scaled, y_train)
print(grid_svc.best_params_)
print(grid_svc.best_estimator_)

grid_svc_predictions = grid_svc.predict(X_test_scaled)
cm = confusion_matrix(y_test, grid_svc_predictions)
accuracy = accuracy_score(y_test, grid_svc_predictions)
class_report = classification_report(y_test, grid_svc_predictions)
sns.heatmap(cm, annot=True)
print('accuracy:', accuracy)
print('classification report:\n', class_report)

knears_params = {"n_neighbors": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}
grid_knears = GridSearchCV(KNeighborsClassifier(), knears_params, refit=True, verbose=4)
grid_knears.fit(X_train_scaled, y_train)
print(grid_knears.best_params_)
print(grid_knears.best_estimator_)

grid_kn_predictions = grid_knears.predict(X_test_scaled)
cm = confusion_matrix(y_test, grid_kn_predictions)
accuracy = accuracy_score(y_test, grid_kn_predictions)
class_report = classification_report(y_test, grid_kn_predictions)
sns.heatmap(cm, annot=True)
print('accuracy:', accuracy)
print('classification report:\n', class_report)

tree_params = {"criterion": ["gini", "entropy"], "max_depth": list(range(2,4,1)), "min_samples_leaf": list(range(5,7,1))}
grid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params, refit=True, verbose=4)
grid_tree.fit(X_train_scaled, y_train)
print(grid_tree.best_params_)
print(grid_tree.best_estimator_)

grid_dt_predictions = grid_tree.predict(X_test_scaled)
cm = confusion_matrix(y_test, grid_dt_predictions)
accuracy = accuracy_score(y_test, grid_dt_predictions)
class_report = classification_report(y_test, grid_dt_predictions)
sns.heatmap(cm, annot=True)
print('accuracy:', accuracy)
print('classification report:\n', class_report)

# randomforest_params = {'bootstrap': [True, False], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None], 'max_features': ['auto', 'sqrt'], 'min_samples_leaf': [1, 2, 4], 'min_samples_split': [2, 5, 10], 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}
randomforest_params = {'bootstrap': [True, False], 'max_depth': [10, 20], 'max_features': ['auto', 'sqrt'], 'min_samples_leaf': [1, 2], 'min_samples_split': [2, 5], 'n_estimators': [200, 400]}
grid_rf = GridSearchCV(RandomForestClassifier(), randomforest_params, refit=True, verbose=4)
grid_rf.fit(X_train_scaled, y_train)
print(grid_rf.best_params_)
print(grid_rf.best_estimator_)

grid_rf_predictions = grid_rf.predict(X_test_scaled)
cm = confusion_matrix(y_test, grid_rf_predictions)
accuracy = accuracy_score(y_test, grid_rf_predictions)
class_report = classification_report(y_test, grid_rf_predictions)
sns.heatmap(cm, annot=True)
print('accuracy:', accuracy)
print('classification report:\n', class_report)

